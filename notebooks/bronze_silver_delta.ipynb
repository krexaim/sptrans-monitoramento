{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6e71bb-0c9f-41ff-9d20-e4580d0ecd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark==3.3.2 in /opt/conda/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: dotenv in /opt/conda/lib/python3.11/site-packages (0.9.9)\n",
      "Collecting pyspark<3.6.0,>=3.5.3 (from delta-spark==3.3.2)\n",
      "  Using cached pyspark-3.5.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib_metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark==3.3.2) (6.8.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (from dotenv) (1.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata>=1.0.0->delta-spark==3.3.2) (3.17.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark==3.3.2) (0.10.9.7)\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.0\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed pyspark-3.5.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta-spark==3.3.2 dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943924ce-4e5d-4142-80ba-6dc551c1435c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abc85a4-2a76-49fb-a7c2-f48de4a3b453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando arquivos arquivos em: s3a://bronze/posicao/2025/11/08/\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('/opt/workspace/.env')\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT_DOCKER\")\n",
    "MINIO_ACCESS = os.getenv(\"MINIO_ROOT_USER\")\n",
    "MINIO_SECRET = os.getenv(\"MINIO_ROOT_PASSWORD\")\n",
    "                           \n",
    "today = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "BRONZE_PATH = f\"s3a://bronze/posicao/{today}/\"\n",
    "SILVER_PATH = \"s3a://silver/posicao/\"\n",
    "print(f\"Buscando arquivos arquivos em: {BRONZE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefca3e6-4b4a-400e-93cf-eadf05471586",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"BronzeToSilver_Delta\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    # Delta Lake\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82235dc0-2357-44f5-b1c2-922fc7bc56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"hr\", StringType(), True),\n",
    "    StructField(\"l\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"c\", StringType(), True),\n",
    "            StructField(\"cl\", IntegerType(), True),\n",
    "            StructField(\"sl\", IntegerType(), True),\n",
    "            StructField(\"lt0\", StringType(), True),\n",
    "            StructField(\"lt1\", StringType(), True),\n",
    "            StructField(\"qv\", IntegerType(), True),\n",
    "            StructField(\"vs\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"p\", IntegerType(), True),\n",
    "                    StructField(\"a\", BooleanType(), True),\n",
    "                    StructField(\"ta\", StringType(), True),\n",
    "                    StructField(\"py\", DoubleType(), True),\n",
    "                    StructField(\"px\", DoubleType(), True),\n",
    "                ])\n",
    "            ), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8489560-fd54-447f-9049-9e73d8682fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pega o arquivo mais recente\n",
    "files_df = spark.read.format(\"binaryFile\").load(BRONZE_PATH)\n",
    "latest_file_row = (\n",
    "    files_df.orderBy(F.col(\"modificationTime\").desc())\n",
    "    .select(\"path\")\n",
    "    .limit(1)\n",
    "    .collect()\n",
    ")\n",
    "latest_file = latest_file_row[0].path\n",
    "print(latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67e092-a302-4c1f-a44c-17317077d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.option(\"mode\", \"PERMISSIVE\").schema(schema).json(latest_file)\n",
    "\n",
    "if df_raw.isEmpty() or df_raw.filter(F.col(\"hr\").isNotNull() & F.col(\"l\").isNotNull()).isEmpty():\n",
    "    print(\"‚ö†Ô∏è Arquivo vazio.\")\n",
    "    spark.stop()\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90c047-ddd9-49ec-b620-48dee1311383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. EXPLODE o array \"l\" linha\n",
    "df_linhas = df_raw.selectExpr(\"hr\", \"inline(l)\")\n",
    "df_linhas.show(2, truncate=False)\n",
    "df_linhas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3f009-51b6-40a8-95f3-ecafc3d7d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. EXPLODE o array \"vs\" ve√≠culos\n",
    "df_veiculos = df_linhas.selectExpr(\n",
    "    \"hr\",\n",
    "    \"c as letreiro\",\n",
    "    \"cl as codigo_linha\",\n",
    "    \"sl as sentido\",\n",
    "    \"lt0 as terminal_inicial\",\n",
    "    \"lt1 as terminal_final\",\n",
    "    \"qv\",\n",
    "    \"inline(vs)\"  # expande os ve√≠culos\n",
    ")\n",
    "df_veiculos.show(3, truncate=False)\n",
    "df_veiculos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067aee7-0548-4864-8521-49236f61707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FILTRAR ONIBUS NAO REGULAR\n",
    "df_filtrado = df_veiculos.filter(\n",
    "    \"codigo_linha IS NOT NULL AND NOT (codigo_linha < 1000 OR letreiro RLIKE 'GUIN|TEST|TST')\"\n",
    ")\n",
    "print(f\"Total de √¥nibus teste/guincho filtrados: {df_veiculos.count() - df_filtrado.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadcffc-6c4f-45d9-be38-be0911873bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Selecionar e renomear colunas √∫teis\n",
    "df_limpo = df_filtrado.select(\n",
    "    \"letreiro\",\n",
    "    \"codigo_linha\",\n",
    "    \"sentido\",\n",
    "    \"terminal_inicial\",\n",
    "    \"terminal_final\",\n",
    "    F.col(\"p\").alias(\"codigo_veiculo\"),\n",
    "    F.col(\"a\").alias(\"acessibilidade\"),\n",
    "    F.to_timestamp(\"ta\").alias(\"ultima_atualizacao\"),\n",
    "    F.col(\"py\").alias(\"latitude\"),\n",
    "    F.col(\"px\").alias(\"longitude\"),\n",
    "    F.to_timestamp(\"hr\").alias(\"hora_referencia\"),\n",
    ")\n",
    "df_limpo.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03c2d9-b5e3-4ab2-a9a6-acbde57d60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remover duplicatas de registros (se houver)\n",
    "df_dedup = df_limpo.dropDuplicates([\"codigo_veiculo\", \"hora_referencia\"])\n",
    "print(f\"Quantidade de registros || Antes: {df_limpo.count()} | Depois: {df_dedup.count()} | # de duplicatas: {df_limpo.count() - df_dedup.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38637f-6063-44a4-b883-b34ad42a7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Corrigir imprecis√£o floating point lat/long e adicionar metadados\n",
    "df_final = (\n",
    "    df_dedup\n",
    "    .withColumn(\"latitude\", F.round(\"latitude\", 6))\n",
    "    .withColumn(\"longitude\", F.round(\"longitude\", 6))\n",
    "    .withColumn(\"data_ref\", F.to_date(\"ultima_atualizacao\"))\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    ")\n",
    "df_final.show(3, truncate=False)\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059021e-973a-4e78-9e16-67f184b47783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b14c5-8c45-428d-a0c0-3e9538c75580",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, SILVER_PATH):\n",
    "    silver_table = DeltaTable.forPath(spark, SILVER_PATH)\n",
    "    (\n",
    "        silver_table.alias(\"tgt\")\n",
    "        .merge(\n",
    "            df_final.alias(\"src\"),\n",
    "            \"tgt.codigo_veiculo = src.codigo_veiculo AND tgt.hora_referencia = src.hora_referencia\"\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "    print(\"üîÅ MERGE incremental conclu√≠do.\")\n",
    "else:\n",
    "    (\n",
    "        df_final.write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"data_ref\")\n",
    "        .save(SILVER_PATH)\n",
    "    )\n",
    "    print(\"üÜï Nova tabela Delta criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25eaa6-08ec-4f57-a7b7-e0d98e16a903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
